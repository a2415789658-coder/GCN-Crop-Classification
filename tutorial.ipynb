{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN Crop Classification Tutorial\n",
    "\n",
    "A step-by-step walkthrough of **crop classification from Sentinel-2 imagery** using a **Graph Convolutional Network (GCN)** built with PyTorch Geometric.\n",
    "\n",
    "This notebook covers the full pipeline: data loading, graph construction, model training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1. Setup & Imports"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = Path('data')\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load & Explore Data\n",
    "\n",
    "The training dataset contains ~24,000 labeled pixels with 23 spectral/vegetation features across 5 crop classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR / 'crop_training_data_5classes_2020.csv')\n",
    "drop_cols = [c for c in ['.geo', 'system:index', 'GCVI'] if c in df.columns]\n",
    "df = df.drop(columns=drop_cols)\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f'Samples: {df.shape[0]}, Columns: {df.shape[1]}')\n",
    "print(f'\\nClass distribution:')\n",
    "print(df['classname'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "colors = ['#e6194b', '#c4a35a', '#3cb44b', '#4363d8', '#f5d742']\n",
    "df['classname'].value_counts().sort_index().plot(kind='bar', color=colors, ax=ax)\n",
    "ax.set_title('Class Distribution', fontsize=14)\n",
    "ax.set_ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Features & Labels\n",
    "\n",
    "We extract 23 numeric features (10 spectral bands + 13 vegetation indices) and use the integer class labels from the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [c for c in df.columns if c not in ['class', 'classname']]\n",
    "class_names = sorted(df['classname'].unique())\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df['class'].values\n",
    "num_classes = len(np.unique(y))\n",
    "num_features = X.shape[1]\n",
    "\n",
    "print(f'Features ({num_features}): {feature_cols}')\n",
    "print(f'Classes ({num_classes}): {class_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Val/Test Split & Normalization\n",
    "\n",
    "70/15/15 stratified split. The `StandardScaler` is fit **only on the training set** to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(X.shape[0])\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.3, stratify=y, random_state=SEED)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=y[temp_idx], random_state=SEED)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X.copy()\n",
    "X_scaled[train_idx] = scaler.fit_transform(X[train_idx])\n",
    "X_scaled[val_idx] = scaler.transform(X[val_idx])\n",
    "X_scaled[test_idx] = scaler.transform(X[test_idx])\n",
    "\n",
    "print(f'Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build KNN Graph\n",
    "\n",
    "GCNs require graph structure. We build a **K-nearest neighbor graph (k=8)** in feature space -- edges connect pixels with similar spectral signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 8\n",
    "knn_adj = kneighbors_graph(X_scaled, n_neighbors=K, mode='connectivity', include_self=False)\n",
    "knn_adj = knn_adj + knn_adj.T\n",
    "knn_adj[knn_adj > 1] = 1\n",
    "\n",
    "rows, cols = knn_adj.nonzero()\n",
    "edge_index = torch.tensor(np.array([rows, cols]), dtype=torch.long)\n",
    "print(f'Nodes: {X_scaled.shape[0]:,}, Edges: {edge_index.shape[1]:,}')\n",
    "print(f'Average degree: {edge_index.shape[1] / X_scaled.shape[0]:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create PyG Data Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.tensor(X_scaled, dtype=torch.float)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "train_mask = torch.zeros(X_scaled.shape[0], dtype=torch.bool); train_mask[train_idx] = True\n",
    "val_mask = torch.zeros(X_scaled.shape[0], dtype=torch.bool); val_mask[val_idx] = True\n",
    "test_mask = torch.zeros(X_scaled.shape[0], dtype=torch.bool); test_mask[test_idx] = True\n",
    "\n",
    "data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor,\n",
    "            train_mask=train_mask, val_mask=val_mask, test_mask=test_mask).to(device)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define the GCN Model\n",
    "\n",
    "```\n",
    "GCNConv(23 -> 128) -> BN -> ReLU -> Dropout(0.5)\n",
    "GCNConv(128 -> 128) -> BN -> ReLU -> Dropout(0.5)\n",
    "GCNConv(128 -> 5)   -> Output logits\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_layers=3, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_dim, hidden_dim))\n",
    "        self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "        self.convs.append(GCNConv(hidden_dim, out_dim))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(len(self.convs) - 1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.convs[-1](x, edge_index)\n",
    "\n",
    "model = GCN(num_features, 128, num_classes, num_layers=3, dropout=0.5).to(device)\n",
    "print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.bincount(y[train_idx])\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum() * num_classes\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "train_losses, val_accs = [], []\n",
    "best_val_acc, patience_counter = 0.0, 0\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(data.x, data.edge_index)[data.val_mask].argmax(1)\n",
    "        val_acc = (pred == data.y[data.val_mask]).float().mean().item()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= 30:\n",
    "        print(f'Early stopping at epoch {epoch}. Best val acc: {best_val_acc:.4f}')\n",
    "        break\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch:3d}  Loss: {loss:.4f}  Val Acc: {val_acc:.4f}')\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "print(f'\\nBest Validation Accuracy: {best_val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 9. Training Visualization"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.plot(train_losses, color='blue')\n",
    "ax1.set_title('Training Loss', fontsize=13)\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss'); ax1.grid(alpha=0.3)\n",
    "ax2.plot(val_accs, color='green')\n",
    "ax2.set_title('Validation Accuracy', fontsize=13)\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy'); ax2.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 10. Evaluate on Test Set"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index)\n",
    "    test_pred = out[data.test_mask].argmax(1).cpu().numpy()\n",
    "    test_true = data.y[data.test_mask].cpu().numpy()\n",
    "\n",
    "idx_to_class = {0: 'Cotton', 1: 'Wheat', 2: 'Fallow', 3: 'Grass', 4: 'Water'}\n",
    "target_names = [idx_to_class[i] for i in range(num_classes)]\n",
    "\n",
    "print('Classification Report:\\n')\n",
    "print(classification_report(test_true, test_pred, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "cm = confusion_matrix(test_true, test_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=target_names, yticklabels=target_names, ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix', fontsize=13)\n",
    "axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('True')\n",
    "\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.3f', cmap='Blues',\n",
    "            xticklabels=target_names, yticklabels=target_names, ax=axes[1])\n",
    "axes[1].set_title('Normalized Confusion Matrix', fontsize=13)\n",
    "axes[1].set_xlabel('Predicted'); axes[1].set_ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Apply the trained model to a full Sentinel-2 raster: `python apply_gcn_to_raster.py`\n",
    "- Explore results in the interactive Streamlit dashboard: `streamlit run app.py`\n",
    "- Experiment with different graph construction methods (spatial vs spectral KNN)\n",
    "- Try varying the number of GCN layers, hidden dimensions, or k-neighbors\n",
    "\n",
    "---\n",
    "\n",
    "*This tutorial is part of the [GCN Crop Classification](https://github.com/Osman-Geomatics93/GCN-Crop-Classification) project.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (geodl)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
